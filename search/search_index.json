{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CNIO Bioinformatics Unit documentation","text":"<p>Welcome to the documentation repository of the CNIO Bioinformatics Unit.</p> <ul> <li> <p>To navigate the documentation just use the menu on the left side of the page.</p> </li> <li> <p>You can check the source code that generates this site here.</p> </li> </ul>"},{"location":"dev/","title":"Guidelines for collaboration when writing code","text":"<p>Here are some guidelines for writing and managing code. You should be familiar with all the concepts in all the text and links in this page (except for the examples for code structure, which are on a need-to-know basis, and the source pages from the extracted paragraphs that you will find pasted inline).</p>"},{"location":"dev/#using-version-control-git","title":"Using version control (git)","text":"<p>We use git for version control. If you're not familiar with git, this is a good place to start, and this is a fantastic follow-up. </p> <p>There are some good interactive resources to learn git in general, and git branching in particular.</p>"},{"location":"dev/#where-to-put-your-code","title":"Where to put your code","text":"<p>We have a github group for the Unit for hosting and managing our code.</p> <p>If you're a member or collaborator of the Bioinformatics Unit and wish to gain access, you simply need to create a GitHub user and either contact a BU member, or create an issue to request access.</p>"},{"location":"dev/#how-to-organise-and-manage-your-code","title":"How to organise and manage your code","text":"<p>In order to facilitate collaboration when more than one person are working on a project, we follow some rules for repository structure and git-related operations (branches, commits, merge requests, etc.).</p>"},{"location":"dev/#project-structure","title":"Project structure","text":"<p>The structure of the repository will be defined by the project owner, ideally following well-established guidelines (see examples for Snakemake, Python, and R.</p>"},{"location":"dev/#git-workflow","title":"git workflow","text":"<p>For git, we follow the GitHub flow model. It's a simple and concise model that (at the time of writing) perfectly suits our requirements.</p>"},{"location":"dev/#commits","title":"Commits","text":"<p>Some more details on how we do commits and merge them into master are available here.</p>"},{"location":"dev/#merge-requests","title":"Merge requests","text":"<p>Start here to understand why merge requests, similarly to commits, need to be \"atomic\".</p> <p>Here are two paragraphs extracted from other sites with some additional details on merge requests:</p> <p>Adapted from https://alexsav.io/git-collaboration-guidelines.html</p> <p>The Pull or Merge Requests (PR) are used to share with your co-developers the code changes you did in your branch and ask them to review it. Here we always do a PR from your branch to the \u2018master\u2019 branch. In a PR, your colleagues will have a view of the differences between the branches and the commits. You must also add a title and a description. The title should be sufficient to understand what is being changed. In the description you should: - make a useful description, - describe what was changed in the pull request, - explain why this PR exists, - make it clear how it does what it sets out to do. E.g: Does it change a column in the database? How is this being done? What happens to the old data? - you may want to use screenshots to demonstrate what has changed if there is a GUI involved in the project.</p> <p>Single Responsibility Principle: The pull request should do only 1 thing.</p> <p>Pull request size: It should be small. The pull request must have a maximum of approximately 250 lines of change.</p> <p>Feature breaking: Whenever it\u2019s possible break pull requests into smaller ones.</p> <p>Adapted from https://yalantis.com/blog/code-review-via-gitlab-merge-requests-code-review-must/</p> <p>To minimize the time spent on reviewing each merge request, you need to have a strategy for code review.</p> <p>In our humble opinion, a good developer is not just someone who follows a programming workflow and writes high-quality code. A good developer knows how to deliver code for review and make the whole code review process effortless for the reviewer.</p> <p>Keep in mind that a good merge request should solve a specific task. We suggest not including more than one feature in one merge request. It would be much better if you created several merge requests instead.</p> <p>As a reviewer, don\u2019t hesitate to pull the source branch and test incoming code by yourself, especially if the merge request contains plenty of changes. Build the project and check that everything works as expected.</p> <p>Also, an important detail in our code review checklist is deleting branches when they\u2019re no longer needed. The responsibility for deleting branches after they\u2019re fully merged lies with the reviewer.</p>"},{"location":"admin/inventory/","title":"Workstations","text":"User Name Area Machine IP chernandez Carolina Hern\u00e1ndez Oliver Grupo de Met\u00e1stasis Cerebral / Unidad de Bioinform\u00e1tica caurel 10.222.112.28 mtress Michael Tress Unidad de Bioinform\u00e1tica andes 10.222.112.32 fsoriano Francisco Javier Soriano D\u00edaz Unidad de Bioinform\u00e1tica tambora 10.222.112.33 lmartinezg Laura Mart\u00ednez g\u00f3mez Unidad de Bioinform\u00e1tica urales 10.222.112.35 dcerdan Daniel Cerd\u00e1n V\u00e9lez Unidad de Bioinform\u00e1tica jura 10.222.112.37 epineiro Elena Pi\u00f1eiro Ya\u00f1ez Unidad de Bioinform\u00e1tica meira 10.222.112.39 dcerdan Daniel Cerd\u00e1n V\u00e9lez Unidad de Bioinform\u00e1tica gazpacho 10.222.112.43 dcerdan Daniel Cerd\u00e1n V\u00e9lez Unidad de Bioinform\u00e1tica maliciosa 10.222.112.53 mjjimenez Mar\u00eda Jos\u00e9 J\u00edmenez Santos Unidad de Bioinform\u00e1tica kilimanjaro 10.222.112.60 lolle Laia Oll\u00e9 Monr\u00e0s Grupo de Computacional Cancer Genomics anakin 10.222.112.68 mjjimenez Mar\u00eda Jos\u00e9 J\u00edmenez Santos Unidad de Bioinform\u00e1tica windu 10.222.112.85 gpiedrafita Gabriel Piedrafita Fern\u00e1ndez Grupo de Carcinog\u00e9nesis Epitelial / Unidad de Bioinform\u00e1tica patterson 10.222.112.97 ggomez Gonzalo G\u00f3mez L\u00f3pez Unidad de Bioinform\u00e1tica quigon 10.222.112.100 jmartinezv Jaime Mart\u00ednez de Villarreal Chico Grupo de Carcinog\u00e9nesis Epitelial gurugu 10.222.112.135 fpozoc Fernando Pozo Ocampo Unidad de Bioinform\u00e1tica jonflash 10.222.112.143 ccarreterop Carlos Carretero Puche Unidad de Investigaci\u00f3n Cl\u00ednica de C\u00e1ncer Pulm\u00f3n H12O-CNIO monete 10.222.112.149 ralvarezd Ruth Alvarez D\u00edaz Grupo de Oncolog\u00eda Experimental terril 10.222.113.25 tdidomenico Tom\u00e1s Di Domenico Unidad de Bioinform\u00e1tica viper 10.222.113.30 sagarcia Santiago Garc\u00eda Mart\u00edn Unidad de Bioinform\u00e1tica azura 10.222.113.32 mmoradiellos Manuel Moradiellos Corpus Grupo de Computacional Cancer Genomics gojira 10.222.113.27 - - - himalaya 10.222.112.49 - - - alytes 10.222.112.109 - - - moriarti 10.222.112.142 - - - tomate 10.222.112.139 - - - drbacterio 10.222.113.129 - - - enllamas -"},{"location":"admin/tasks/","title":"Setting up workstation users","text":"<p>Note</p> <p>If it's a new user, you'll need to send a ticket (incidencia) to IT for them to a) enable the user for Linux and b) create the shared home (specify both).</p> <p>Warning</p> <p>The cnio username (\"xxxx\" in \"xxxx@cnio.es\") should not exist locally on the machine or it will clash with the remote one. If you used the same username for the installation of the OS you will first need to remove it from the local machine.</p>"},{"location":"admin/tasks/#ldap","title":"LDAP","text":""},{"location":"admin/tasks/#install-required-packages-leave-all-options-as-default-when-prompted","title":"Install required packages (leave all options as default when prompted)","text":"<pre><code>$ sudo apt-get update\n$ sudo apt-get -y install libnss-ldap libpam-ldap ldap-utils nscd\n</code></pre>"},{"location":"admin/tasks/#update-etcnsswitchconf","title":"Update /etc/nsswitch.conf","text":"<pre><code>passwd:         compat ldap\ngroup:          compat ldap\nshadow:         compat ldap\n</code></pre>"},{"location":"admin/tasks/#replace-contents-in-etcldapconf","title":"Replace contents in /etc/ldap.conf","text":"<pre><code>HOST cnio.es\n#***** HOST seth.cnio.es\n\n#BASE DC=cnio,DC=es\nBASE OU=Usuarios,OU=Programa Biologia Estructural,OU=CNIO.ES,DC=cnio,DC=es\n\nbinddn cn=bioldap,cn=Users,dc=cnio,dc=es\nbindpw #####REDACTED######\n\nnss_base_passwd OU=Usuarios,OU=Programa Biologia Estructural,OU=CNIO.ES,DC=cnio,DC=es?sub\nnss_base_passwd OU=Usuarios,OU=Programa Biotecnologia,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_passwd OU=Usuarios,OU=Programa Terapias Experimentales,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_passwd DC=cnio,DC=es?sub\nnss_base_shadow OU=Usuarios,OU=Programa Biologia Estructural,OU=CNIO.ES,DC=cnio,DC=es?sub\nnss_base_shadow OU=Usuarios,OU=Programa Biotecnologia,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_shadow OU=Usuarios,OU=Programa Terapias Experimentales,OU=CNIO.ES,DC=cnio,DC=es\nnss_base_shadow DC=cnio,DC=es?sub\nnss_base_group OU=PBE,OU=ACLs,DC=cnio,DC=es?sub\nnss_map_objectclass posixAccount User\nnss_map_objectclass shadowAccount User\nnss_map_attribute uid sAMAccountName\nnss_map_attribute uidNumber msSFU30UidNumber\nnss_map_attribute gidNumber msSFU30GidNumber\nnss_map_attribute homeDirectory msSFU30HomeDirectory\nnss_map_attribute loginShell msSFU30LoginShell\nnss_map_objectclass posixGroup Group\nnss_map_attribute cn sAMAccountName\npam_login_attribute sAMAccountName\npam_filter objectclass=user\npam_password ad  \nnss_initgroups_ignoreusers avahi,avahi-autoipd,backup,bin,colord,daemon,debian-spamd,dhcpd,dnsmasq,elasticsearch,games,gdm,gnats,guest-ngHhtq,hplip,irc,kernoops,libuuid,lightdm,list,lp,mail,man,messagebus,mysql,nagios,news,postfix,postgres,proxy,pulse,root,rstudio-server,rtkit,saned,sgeadmin,speech-dispatcher,sshd,statd,sync,sys,syslog,tftp,usbmux,uucp,whoopsie,www-data\n</code></pre>"},{"location":"admin/tasks/#restart-nscd-service","title":"Restart nscd service","text":"<pre><code>$ sudo service nscd restart\n</code></pre>"},{"location":"admin/tasks/#verify-ldap-login","title":"Verify LDAP login","text":"<pre><code>$ getent passwd ldapuser\n\nldapuser:x:9999:100:Test LdapUser:/home/ldapuser:/bin/bash\n</code></pre>"},{"location":"admin/tasks/#cnio-shared-homes","title":"CNIO shared homes","text":"<p>Home mounting was originally done with autofs. It does not play well with systemd and the long delays on network uplinks, so it's a bit of a mess in Ubuntu (tested on 18). The \"classic\" fstab mounting is therefore preferred.</p> <p>Warning</p> <p>The \"classic\" and the autofs mounts are mutually exclusive. You need to set up one or the other. Otherwise you'll have conflicts and problems will arise.</p>"},{"location":"admin/tasks/#classic-mount","title":"Classic mount","text":""},{"location":"admin/tasks/#install-nfs","title":"Install nfs","text":"<p>Install nfs-common if not already available:</p> <pre><code>apt-get install nfs-common\n</code></pre>"},{"location":"admin/tasks/#edit-etcfstab","title":"Edit /etc/fstab","text":"<pre><code>lando.cnio.es:/homes/&lt;user&gt; /home/&lt;user&gt; nfs  auto,noatime,nolock,bg,nfsvers=3,tcp,intr,_netdev,x-systemd.automount,x-systemd.after=network-online.target,x-systemd.device-timeout=240      0       0\n</code></pre>"},{"location":"admin/tasks/#autofs","title":"autofs","text":""},{"location":"admin/tasks/#install-required-packages","title":"Install required packages","text":"<pre><code>$ sudo apt-get -y install autofs\n</code></pre>"},{"location":"admin/tasks/#add-a-home-alias-to-etcautomaster-if-its-not-there-yet","title":"Add a /home alias to /etc/auto.master (if it's not there yet)","text":"<pre><code>$ vi /etc/auto.master\n...\n/home /etc/auto.home\n...\n</code></pre>"},{"location":"admin/tasks/#add-the-user-to-the-home-automount-list","title":"Add the user to the home automount list","text":"<pre><code>$ vi /etc/auto.home\n...\nusername      lando.cnio.es:/homes/username\n...\n</code></pre>"},{"location":"hpc/admin/","title":"Cluster administration","text":""},{"location":"hpc/admin/#rack-layout","title":"Rack layout","text":"<p>Diagram can be edited with diagrams.net using the source code available here</p>"},{"location":"hpc/admin/#checking-and-restarting-nodes","title":"Checking and restarting nodes","text":"<p>For a full list of all available nodes, type:</p> <pre><code>scontrol show node\n</code></pre> <p>A given node status can be checked out with:</p> <pre><code>scontrol show node NODENAME\n</code></pre> <p>Sometimes, a node state will change to DRAIN for multiple reasons: i.e. slurm could not gracefully kill a job on a given node. Drained nodes won't handle incoming jobs until they are manually reset. To do so, the following command must be typed with admin privileges: </p> <pre><code>scontrol update NodeName=NODENAME State=RESUME\n</code></pre>"},{"location":"hpc/usage/","title":"The CNIO HPC cluster","text":"<p>These are some guidelines to using our HPC cluster and the Slurm workflow system.</p>"},{"location":"hpc/usage/#cluster-resources","title":"Cluster resources","text":""},{"location":"hpc/usage/#compute-nodes","title":"Compute nodes","text":"<p>The cluster currently features 8 compute nodes with the following configurations:</p> Node CPUs RAM GPUs bc001 24 32Gb - bc00[2-7] 52 512Gb - hm001 224 2Tb - gp001 112 768Gb 2 x Nvidia A100 80Gb <p></p>"},{"location":"hpc/usage/#cluster-usage","title":"Cluster usage","text":""},{"location":"hpc/usage/#introduction-to-hpc","title":"Introduction to HPC","text":"<p>See this tutorial for an introduction to HPC concepts and usage (highly recommended if you have little or no experience in using HPC environments).</p>"},{"location":"hpc/usage/#requesting-access","title":"Requesting access","text":"<p>Please fill out the access request form in order to  request access to the cluster.</p>"},{"location":"hpc/usage/#accessing-the-cluster","title":"Accessing the cluster","text":"<p>The cluster's hostname is <code>cluster1.cnio.es</code> (<code>cluster1</code> for short), and can be accessed via SSH.</p> <p>Note</p> <p>If you'd like to SSH into the cluster through the CNIO VPN you will need to ask IT for VPN access.</p>"},{"location":"hpc/usage/#mailing-list","title":"Mailing list","text":"<p>Important notifications about the cluster are sent to the CNIO HPC mailing list. Make sure to subscribe using this link to stay up to date. The list has very low traffic.</p> <p>Note</p> <p>Do subscribe to the list!</p>"},{"location":"hpc/usage/#storage","title":"Storage","text":""},{"location":"hpc/usage/#home-directories","title":"Home directories","text":"<p>You have an initial allocation of 300Gb in your home directory.</p> <p>This space is well suited for for storing software (e.g. conda), configuration files, etc.</p> <p>It is however not quick enough to store the input or output files of your analyses. Using your home for this will likely hang your jobs and corrupt your files, and you should use the scratch space instead.</p>"},{"location":"hpc/usage/#scratch-space","title":"Scratch space","text":"<p>You have an inital allocation of 1Tb in your \"scratch\" directory, located at <code>/storage/scratch01/users/&lt;yourusername&gt;/</code>.</p> <p>Input and output of any computation that you do in the cluster should be stored there.</p>"},{"location":"hpc/usage/#checking-your-quotas","title":"Checking your quotas","text":"<p>You can check your current quotas with the following commands:</p> <pre><code>$ zfs get userquota@$(whoami) homepool/home #check your home quota\n\n$ lfs quota -u $(whoami) -h /storage/scratch01/ #check your scratch quota\n</code></pre>"},{"location":"hpc/usage/#data-availability-and-security","title":"Data availability and security","text":"<p>Scratch space is to be considered \"volatile\": you should copy your input files, execute, copy your output files, and delete everything.</p> <p>Home directories are not volatile, but data safety is not guaranteed in the long term (so keep backups).</p>"},{"location":"hpc/usage/#copying-data-tofrom-the-cluster","title":"Copying data to/from the cluster","text":"<p>The most efficient way of copying large amounts of data is by using <code>rsync</code>. <code>rsync</code> allows you to resume a transfer in case something goes wrong.</p> <p>To transfer a directory to the cluster using <code>rsync</code> you would do something like:</p> <pre><code>rsync -avx --progress mydirectory/ cluster1:/storage/scratch01/myuser/mydirectory/\n</code></pre> <p>Note</p> <p>Pay attention to the trailing slashes, which completely change rsync's behaviour if missing.</p> <p>For small transfers you could also use <code>scp</code> if you prefer:</p> <pre><code>scp -r mydirectory/ cluster1:/storage/scratch01/myuser/\n</code></pre> <p>In both cases you can revert the order of the local and remote directory to copy from the cluster to your local computer instead.</p>"},{"location":"hpc/usage/#submitting-jobs","title":"Submitting jobs","text":"<p>Warning</p> <p>As a general rule, no heavy processes should be run directly on the login nodes. Instead you should use the \"sbatch\" or \"srun\" commands to send them to the compute nodes. See examples below.</p> <p>The command structure to send a job to the queue is the following:</p> <pre><code>sbatch -o &lt;logfile&gt; -e &lt;errfile&gt; -J &lt;jobname&gt; -c &lt;ncores&gt; --mem=&lt;total_memory&gt;G \\\n    -t&lt;time_minutes&gt; --wrap \"&lt;command&gt;\"\n</code></pre> <p>For example, to submit the command <code>bwa index mygenome.fasta</code> as a job: </p> <pre><code>sbatch -o log.txt -e error.txt -J index_genome -c 1 --mem=4G -t120 \\\n    --wrap \"bwa index mygenome.fasta\"\n</code></pre>"},{"location":"hpc/usage/#job-resources","title":"Job resources","text":"<p><code>-c</code>, <code>--mem</code>, and <code>-t</code> tell the system how many cores, RAM memory, and time your job will need.</p>"},{"location":"hpc/usage/#time-limits","title":"Time limits","text":"<p>Note</p> <p>Ideally, you should aim at your jobs having a duration of between 1 and 8 hours. Shorter jobs may cause too much scheduling overhead, and longer jobs will make it harder for the scheduler to optimally schedule jobs into the queues.</p> <p>If no time limit is specified, a default of 30 minutes will be assigned automatically.</p> <p>The \"short\" queue has a limit of 2 hours per job and the highest priority (i.e. jobs in this queue will run sooner compared to other queues).</p> <p>The \"main\" queue has a limit of 24 hours per job and medium priority. </p> <p>The \"long\" queue has a limit of 168 hours (7 days) per job and 4 concurrent jobs, and the lowest priority.</p>"},{"location":"hpc/usage/#resources-and-their-effect-on-job-priority","title":"Resources and their effect on job priority","text":"<p>The resources you request for a job will influence the chances that such job has to enter the queue, compared to others: the more resources you request, the longer you may have to wait for those resources to be available.</p> <p>In addition, the future priority of your jobs will also be influenced by the resources you request (not use, request, even if you don't use them in the end): the more you request, the less priority you'll have for future jobs.</p> <p>If one of your jobs has lower priority than another one, but its running time would not delay that higher priority one from entering the queue and there are enough resources for it, your job could enter the queue first. That's why it's important to try and assign reasonably accurate time limits (a.k.a. walltimes) to your jobs.</p> <p>You can check how efficiently a job used its assigned resources with the <code>seff &lt;jobid&gt;</code> command (once the job is finished).</p>"},{"location":"hpc/usage/#snakemake-profile","title":"Snakemake profile","text":"<p>The cluster features a Snakemake profile that allows for the automatic submision and management of jobs. To use it, just add the <code>--profile $SMK_PROFILE_SLURM</code> argument to your Snakemake command.</p> <p>Note</p> <p>In order to indicate the resources required by each Snakemake rule you should use the threads and resources options.</p> <p>Note</p> <p>The Snakemake command will remain active while your jobs run, so it's recommended that you launch it inside a detachable terminal emulator (e.g. GNU Screen) so you can disconnect from the cluster and keep your jobs running.</p>"},{"location":"hpc/usage/#interactive-sessions","title":"Interactive sessions","text":"<p>During development and testing you may want to be able to run commands interactively. You can request an interactive session, which will run on a compute node, using the <code>srun</code> command with the <code>--pty</code> argument.</p> <p>For example, to request a 2-hour session with 4Gb RAM and 2 CPUs, you would do:</p> <p><code>srun --mem=4096 -c 2 -t 120 --pty /bin/bash</code></p> <p>Note</p> <p>Interactive sessions are limited to a maximum of 120 minutes.</p>"},{"location":"hpc/usage/#gpus","title":"GPUs","text":"<p>Warning</p> <p>GPU access is currently being tested and should be requested to hpc-admin@lists.cnio.es.</p> <p>All GPU-related information is subject to change.</p> <p>The cluster features two Nvidia A100 GPUs, each split into 4 instances with 20Gb of VRAM, for a total of 8x20Gb instances.</p> <p>To run a command using GPUs you need to specify the <code>gpu</code> partition, and request the number of instances you require using the <code>--gres=gpu:1g.20gb:&lt;number_of_instances&gt;</code> argument of sbatch. Here's an example to request two instances:</p> <p><code>sbatch -p gpu --gres=gpu:1g.20gb:2 --wrap \"python train_my_net.py\"</code></p>"},{"location":"hpc/usage/#installing-software","title":"Installing software","text":"<p>Software management is left up to the user, and we recommend doing it by installing mambaforge and the bioconda channels.</p> <p>If you're completely unfamiliar with conda and bioconda, we recommend following this tutorial. Notice that the tutorial  uses miniconda, which is equivalent to the recommended mambaforge.</p> <p>Note</p> <p>Remember to use your home directory to install software (including conda), as lustre performs poorly when reading small files often.</p>"}]}